{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/codeflamer/Airbnb-clone/blob/main/notebooks/Chapter%2004%20-%20Intro_to_Prompt_Engineering_Tips_and_Tricks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UankOoEqfvT5",
        "outputId": "149e4343-f6dc-4277-e564-84249ceb229e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q langchain==0.0.208 openai==0.27.8 python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpnwP9Z6gAtB",
        "outputId": "e61fabbd-9c0f-4499-b549-5854e307d444"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "\n",
        "!echo \"OPENAI_API_KEY='<OPENAI_API_KEY>'\" > .env\n",
        "\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0Qehe870ht2"
      },
      "source": [
        "# Story Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sr7-jcJl0J6Y",
        "outputId": "f0021277-633c-42e7-8aa7-27a1bb83482f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "embark on a quest to find the mystical cheese of legends. Along the way, he encountered clever challenges and made unlikely friends with a wise old owl and a mischievous squirrel. The journey tested his bravery and determination, but Benjamin never gave up.\n"
          ]
        }
      ],
      "source": [
        "import openai\n",
        "\n",
        "prompt_system = \"You are a helpful assistant whose goal is to help write stories.\"\n",
        "\n",
        "prompt = \"\"\"Continue the following story. Write no more than 50 words.\n",
        "\n",
        "Once upon a time, in a world where animals could speak, a courageous \\\n",
        "mouse named Benjamin decided to\"\"\"\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": prompt_system},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(response.choices[0]['message']['content'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGH6rUOG0i07"
      },
      "source": [
        "# Product Description"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9fWIO_00EwL",
        "outputId": "c4aaa71c-7c1f-497a-aeae-0a85f301dab3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Indulge in the refined elegance of our limited-edition fountain pen, meticulously handcrafted from lustrous rosewood and accented with opulent gold detailing. This exquisite piece of artistry embodies sophistication and luxury, destined to elevate your writing experience to new heights.\n"
          ]
        }
      ],
      "source": [
        "import openai\n",
        "\n",
        "prompt_system = \"\"\"You are a helpful assistant whose goal is to help write \\\n",
        "product descriptions.\"\"\"\n",
        "\n",
        "prompt = \"\"\"Write a captivating product description for a luxurious, handcrafted\\\n",
        ", limited-edition fountain pen made from rosewood and gold.\n",
        "Write no more than 50 words.\"\"\"\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": prompt_system},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(response.choices[0]['message']['content'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itmZFeTG0lQv"
      },
      "source": [
        "# Zero-Shot Prompting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swbFmvyh0Bbu",
        "outputId": "ffc30ef3-4b04-4ebc-ab1e-1583556714db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Summer arrives with a golden glow,\n",
            "Warm sun on skin, a gentle breeze to show,\n",
            "Days filled with laughter, evenings aglow,\n",
            "In this season of bliss, memories flow.\n"
          ]
        }
      ],
      "source": [
        "import openai\n",
        "\n",
        "prompt_system = \"You are a helpful assistant whose goal is to write short poems.\"\n",
        "\n",
        "prompt = \"\"\"Write a short poem about {topic}.\"\"\"\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": prompt_system},\n",
        "        {\"role\": \"user\", \"content\": prompt.format(topic=\"summer\")}\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(response.choices[0]['message']['content'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pwz-8Gdo0qri"
      },
      "source": [
        "# In-Context Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPLaRGsRz8uT",
        "outputId": "69ad4cfc-c5f1-4a6e-ab71-f60924db5ffe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Golden sun up high,\n",
            "Laughter echoes in the sky,\n",
            "Summer days fly by.\n"
          ]
        }
      ],
      "source": [
        "import openai\n",
        "\n",
        "prompt_system = \"You are a helpful assistant whose goal is to write short poems.\"\n",
        "\n",
        "prompt = \"\"\"Write a short poem about {topic}.\"\"\"\n",
        "\n",
        "examples = {\n",
        "    \"nature\": \"\"\"Birdsong fills the air,\\nMountains high and valleys \\\n",
        "deep,\\nNature's music sweet.\"\"\",\n",
        "    \"winter\": \"\"\"Snow blankets the ground,\\nSilence is the only \\\n",
        "sound,\\nWinter's beauty found.\"\"\"\n",
        "}\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": prompt_system},\n",
        "        {\"role\": \"user\", \"content\": prompt.format(topic=\"nature\")},\n",
        "        {\"role\": \"assistant\", \"content\": examples[\"nature\"]},\n",
        "        {\"role\": \"user\", \"content\": prompt.format(topic=\"winter\")},\n",
        "        {\"role\": \"assistant\", \"content\": examples[\"winter\"]},\n",
        "        {\"role\": \"user\", \"content\": prompt.format(topic=\"summer\")}\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(response.choices[0]['message']['content'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48RPzI1S0r7j"
      },
      "source": [
        "# Few Shot Prompting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_coZjXhz0L1",
        "outputId": "27a5e6da-8dce-46bb-c5c9-7091d91b85a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Color: purple\n",
            "Emotion: royalty or luxury\n"
          ]
        }
      ],
      "source": [
        "from langchain import PromptTemplate, FewShotPromptTemplate, LLMChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "# Initialize LLM\n",
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "examples = [\n",
        "    {\"color\": \"red\", \"emotion\": \"passion\"},\n",
        "    {\"color\": \"blue\", \"emotion\": \"serenity\"},\n",
        "    {\"color\": \"green\", \"emotion\": \"tranquility\"},\n",
        "]\n",
        "\n",
        "example_formatter_template = \"\"\"\n",
        "Color: {color}\n",
        "Emotion: {emotion}\\n\n",
        "\"\"\"\n",
        "example_prompt = PromptTemplate(\n",
        "    input_variables=[\"color\", \"emotion\"],\n",
        "    template=example_formatter_template,\n",
        ")\n",
        "\n",
        "few_shot_prompt = FewShotPromptTemplate(\n",
        "    examples=examples,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=\"\"\"Here are some examples of colors and the emotions associated with \\\n",
        "them:\\n\\n\"\"\",\n",
        "    suffix=\"\"\"\\n\\nNow, given a new color, identify the emotion associated with \\\n",
        "it:\\n\\nColor: {input}\\nEmotion:\"\"\",\n",
        "    input_variables=[\"input\"],\n",
        "    example_separator=\"\\n\",\n",
        ")\n",
        "\n",
        "formatted_prompt = few_shot_prompt.format(input=\"purple\")\n",
        "\n",
        "# Create the LLMChain for the prompt\n",
        "chain = LLMChain(llm=llm, prompt=PromptTemplate(template=formatted_prompt,\n",
        "input_variables=[]))\n",
        "\n",
        "# Run the LLMChain to get the AI-generated emotion associated with the input\n",
        "# color\n",
        "response = chain.run({})\n",
        "\n",
        "print(\"Color: purple\")\n",
        "print(\"Emotion:\", response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1cEIdTO0vi2"
      },
      "source": [
        "# Role Prompting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iuflOySdgCsv",
        "outputId": "2203d32c-d46d-4f93-ec5e-7094a1d7ce9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Theme: interstellar travel\n",
            "Year: 3030\n",
            "AI-generated song title: \n",
            "\"Journey to the Stars: 3030\"\n"
          ]
        }
      ],
      "source": [
        "from langchain import PromptTemplate, LLMChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "# Initialize LLM\n",
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "template = \"\"\"\n",
        "As a futuristic robot band conductor, I need you to help me come up with a song title.\n",
        "What's a cool song title for a song about {theme} in the year {year}?\n",
        "\"\"\"\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"theme\", \"year\"],\n",
        "    template=template,\n",
        ")\n",
        "\n",
        "# Create the LLMChain for the prompt\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "# Input data for the prompt\n",
        "input_data = {\"theme\": \"interstellar travel\", \"year\": \"3030\"}\n",
        "\n",
        "# Run the LLMChain to get the AI-generated song title\n",
        "response = chain.run(input_data)\n",
        "\n",
        "print(\"Theme: interstellar travel\")\n",
        "print(\"Year: 3030\")\n",
        "print(\"AI-generated song title:\", response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_R56C3dgK49",
        "outputId": "188de295-0d9d-496d-b4d9-f1b2c58ce684"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Color: purple\n",
            "Emotion:  creativity\n"
          ]
        }
      ],
      "source": [
        "from langchain import FewShotPromptTemplate\n",
        "\n",
        "examples = [\n",
        "    {\"color\": \"red\", \"emotion\": \"passion\"},\n",
        "    {\"color\": \"blue\", \"emotion\": \"serenity\"},\n",
        "    {\"color\": \"green\", \"emotion\": \"tranquility\"},\n",
        "]\n",
        "\n",
        "example_formatter_template = \"\"\"\n",
        "Color: {color}\n",
        "Emotion: {emotion}\\n\n",
        "\"\"\"\n",
        "example_prompt = PromptTemplate(\n",
        "    input_variables=[\"color\", \"emotion\"],\n",
        "    template=example_formatter_template,\n",
        ")\n",
        "\n",
        "few_shot_prompt = FewShotPromptTemplate(\n",
        "    examples=examples,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=\"Here are some examples of colors and the emotions associated with them:\\n\\n\",\n",
        "    suffix=\"\\n\\nNow, given a new color, identify the emotion associated with it:\\n\\nColor: {input}\\nEmotion:\",\n",
        "    input_variables=[\"input\"],\n",
        "    example_separator=\"\\n\",\n",
        ")\n",
        "\n",
        "formatted_prompt = few_shot_prompt.format(input=\"purple\")\n",
        "\n",
        "# Create the LLMChain for the prompt\n",
        "chain = LLMChain(llm=llm, prompt=PromptTemplate(template=formatted_prompt, input_variables=[]))\n",
        "\n",
        "# Run the LLMChain to get the AI-generated emotion associated with the input color\n",
        "response = chain.run({})\n",
        "\n",
        "print(\"Color: purple\")\n",
        "print(\"Emotion:\", response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTUMpai0083e"
      },
      "source": [
        "# Chain Prompting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5ZqopkbhDsi",
        "outputId": "a842da0e-3a79-444f-bd3a-1bc754cd5eff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scientist: Albert Einstein\n",
            "Fact: \n",
            "Albert Einstein's theory of general relativity is a theory of gravitation that states that the gravitational force between two objects is a result of the curvature of spacetime caused by the presence of mass and energy. It explains the phenomenon of gravity as a result of the warping of space and time by matter and energy.\n"
          ]
        }
      ],
      "source": [
        "# Prompt 1\n",
        "template_question = \"\"\"What is the name of the famous scientist who developed the theory of general relativity?\n",
        "Answer: \"\"\"\n",
        "prompt_question = PromptTemplate(template=template_question, input_variables=[])\n",
        "\n",
        "# Prompt 2\n",
        "template_fact = \"\"\"Provide a brief description of {scientist}'s theory of general relativity.\n",
        "Answer: \"\"\"\n",
        "prompt_fact = PromptTemplate(input_variables=[\"scientist\"], template=template_fact)\n",
        "\n",
        "# Create the LLMChain for the first prompt\n",
        "chain_question = LLMChain(llm=llm, prompt=prompt_question)\n",
        "\n",
        "# Run the LLMChain for the first prompt with an empty dictionary\n",
        "response_question = chain_question.run({})\n",
        "\n",
        "# Extract the scientist's name from the response\n",
        "scientist = response_question.strip()\n",
        "\n",
        "# Create the LLMChain for the second prompt\n",
        "chain_fact = LLMChain(llm=llm, prompt=prompt_fact)\n",
        "\n",
        "# Input data for the second prompt\n",
        "input_data = {\"scientist\": scientist}\n",
        "\n",
        "# Run the LLMChain for the second prompt\n",
        "response_fact = chain_fact.run(input_data)\n",
        "\n",
        "print(\"Scientist:\", scientist)\n",
        "print(\"Fact:\", response_fact)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErdKr6EI1Do1"
      },
      "source": [
        "# Bad Prompt Practices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "UNsOOLC2gkfJ",
        "outputId": "169a1488-762a-4ead-9e49-b5f1b087d9d7"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Tell me something about dogs.'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "template = \"Tell me something about {topic}.\"\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"topic\"],\n",
        "    template=template,\n",
        ")\n",
        "prompt.format(topic=\"dogs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOByBkJVhaBL",
        "outputId": "34ffbf6f-70a8-4f8f-d594-06181f36f8ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scientist: Albert Einstein\n",
            "Fact:  Albert Einstein was a vegetarian and an advocate for animal rights. He was also a pacifist and a socialist, and he was a strong supporter of the civil rights movement. He was also a passionate violinist and a lover of sailing.\n"
          ]
        }
      ],
      "source": [
        "# Prompt 1\n",
        "template_question = \"\"\"What is the name of the famous scientist who developed the theory of general relativity?\n",
        "Answer: \"\"\"\n",
        "prompt_question = PromptTemplate(template=template_question, input_variables=[])\n",
        "\n",
        "# Prompt 2\n",
        "template_fact = \"\"\"Tell me something interesting about {scientist}.\n",
        "Answer: \"\"\"\n",
        "prompt_fact = PromptTemplate(input_variables=[\"scientist\"], template=template_fact)\n",
        "\n",
        "# Create the LLMChain for the first prompt\n",
        "chain_question = LLMChain(llm=llm, prompt=prompt_question)\n",
        "\n",
        "# Run the LLMChain for the first prompt with an empty dictionary\n",
        "response_question = chain_question.run({})\n",
        "\n",
        "# Extract the scientist's name from the response\n",
        "scientist = response_question.strip()\n",
        "\n",
        "# Create the LLMChain for the second prompt\n",
        "chain_fact = LLMChain(llm=llm, prompt=prompt_fact)\n",
        "\n",
        "# Input data for the second prompt\n",
        "input_data = {\"scientist\": scientist}\n",
        "\n",
        "# Run the LLMChain for the second prompt\n",
        "response_fact = chain_fact.run(input_data)\n",
        "\n",
        "print(\"Scientist:\", scientist)\n",
        "print(\"Fact:\", response_fact)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKzhyWSEiG9H",
        "outputId": "25e50f68-1cec-444f-ec02-a506f73e6b80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Genres: jazz pop rock\n",
            "Fact: \n",
            "Jazz, pop, and rock are all genres of popular music that have been around for decades. They all have distinct sounds and styles, and have influenced each other in various ways. Jazz is often characterized by improvisation, complex harmonies, and syncopated rhythms. Pop music is typically more accessible and often features catchy melodies and hooks. Rock music is often characterized by distorted guitars, heavy drums, and powerful vocals.\n"
          ]
        }
      ],
      "source": [
        "# Prompt 1\n",
        "template_question = \"\"\"What are some musical genres?\n",
        "Answer: \"\"\"\n",
        "prompt_question = PromptTemplate(template=template_question, input_variables=[])\n",
        "\n",
        "# Prompt 2\n",
        "template_fact = \"\"\"Tell me something about {genre1}, {genre2}, and {genre3} without giving any specific details.\n",
        "Answer: \"\"\"\n",
        "prompt_fact = PromptTemplate(input_variables=[\"genre1\", \"genre2\", \"genre3\"], template=template_fact)\n",
        "\n",
        "# Create the LLMChain for the first prompt\n",
        "chain_question = LLMChain(llm=llm, prompt=prompt_question)\n",
        "\n",
        "# Run the LLMChain for the first prompt with an empty dictionary\n",
        "response_question = chain_question.run({})\n",
        "\n",
        "# Assign three hardcoded genres\n",
        "genre1, genre2, genre3 = \"jazz\", \"pop\", \"rock\"\n",
        "\n",
        "# Create the LLMChain for the second prompt\n",
        "chain_fact = LLMChain(llm=llm, prompt=prompt_fact)\n",
        "\n",
        "# Input data for the second prompt\n",
        "input_data = {\"genre1\": genre1, \"genre2\": genre2, \"genre3\": genre3}\n",
        "\n",
        "# Run the LLMChain for the second prompt\n",
        "response_fact = chain_fact.run(input_data)\n",
        "\n",
        "print(\"Genres:\", genre1, genre2, genre3)\n",
        "print(\"Fact:\", response_fact)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MVvNNQA1M2F"
      },
      "source": [
        "# Tips for Effective Prompting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-MLn4suilQy",
        "outputId": "6b7f4c3e-981a-4744-b14c-59fcd13e82b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User Query: What are some tips for improving communication skills?\n",
            "AI Response:  Practice active listening, be mindful of your body language, and be open to constructive feedback.\n"
          ]
        }
      ],
      "source": [
        "examples = [\n",
        "    {\n",
        "        \"query\": \"What's the secret to happiness?\",\n",
        "        \"answer\": \"Finding balance in life and learning to enjoy the small moments.\"\n",
        "    }, {\n",
        "        \"query\": \"How can I become more productive?\",\n",
        "        \"answer\": \"Try prioritizing tasks, setting goals, and maintaining a healthy work-life balance.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "example_template = \"\"\"\n",
        "User: {query}\n",
        "AI: {answer}\n",
        "\"\"\"\n",
        "\n",
        "example_prompt = PromptTemplate(\n",
        "    input_variables=[\"query\", \"answer\"],\n",
        "    template=example_template\n",
        ")\n",
        "\n",
        "prefix = \"\"\"The following are excerpts from conversations with an AI\n",
        "life coach. The assistant provides insightful and practical advice to the users' questions. Here are some\n",
        "examples:\n",
        "\"\"\"\n",
        "\n",
        "suffix = \"\"\"\n",
        "User: {query}\n",
        "AI: \"\"\"\n",
        "\n",
        "few_shot_prompt_template = FewShotPromptTemplate(\n",
        "    examples=examples,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=prefix,\n",
        "    suffix=suffix,\n",
        "    input_variables=[\"query\"],\n",
        "    example_separator=\"\\n\\n\"\n",
        ")\n",
        "\n",
        "# Create the LLMChain for the few-shot prompt template\n",
        "chain = LLMChain(llm=llm, prompt=few_shot_prompt_template)\n",
        "\n",
        "# Define the user query\n",
        "user_query = \"What are some tips for improving communication skills?\"\n",
        "\n",
        "# Run the LLMChain for the user query\n",
        "response = chain.run({\"query\": user_query})\n",
        "\n",
        "print(\"User Query:\", user_query)\n",
        "print(\"AI Response:\", response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kXPDhFq4i5Ep"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}